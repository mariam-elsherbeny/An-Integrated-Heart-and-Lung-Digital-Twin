{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b17c6e05",
   "metadata": {
    "collapsed": true,
    "execution": {
     "iopub.execute_input": "2025-04-13T12:23:46.697093Z",
     "iopub.status.busy": "2025-04-13T12:23:46.696653Z",
     "iopub.status.idle": "2025-04-13T12:23:58.020585Z",
     "shell.execute_reply": "2025-04-13T12:23:58.019506Z"
    },
    "jupyter": {
     "outputs_hidden": true
    },
    "papermill": {
     "duration": 11.330865,
     "end_time": "2025-04-13T12:23:58.022239",
     "exception": false,
     "start_time": "2025-04-13T12:23:46.691374",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting segmentation_models_pytorch\r\n",
      "  Downloading segmentation_models_pytorch-0.4.0-py3-none-any.whl.metadata (32 kB)\r\n",
      "Collecting efficientnet-pytorch>=0.6.1 (from segmentation_models_pytorch)\r\n",
      "  Downloading efficientnet_pytorch-0.7.1.tar.gz (21 kB)\r\n",
      "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\r\n",
      "Requirement already satisfied: huggingface-hub>=0.24 in /usr/local/lib/python3.10/dist-packages (from segmentation_models_pytorch) (0.29.0)\r\n",
      "Requirement already satisfied: numpy>=1.19.3 in /usr/local/lib/python3.10/dist-packages (from segmentation_models_pytorch) (1.26.4)\r\n",
      "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.10/dist-packages (from segmentation_models_pytorch) (11.0.0)\r\n",
      "Collecting pretrainedmodels>=0.7.1 (from segmentation_models_pytorch)\r\n",
      "  Downloading pretrainedmodels-0.7.4.tar.gz (58 kB)\r\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.8/58.8 kB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\r\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from segmentation_models_pytorch) (1.17.0)\r\n",
      "Requirement already satisfied: timm>=0.9 in /usr/local/lib/python3.10/dist-packages (from segmentation_models_pytorch) (1.0.12)\r\n",
      "Requirement already satisfied: torch>=1.8 in /usr/local/lib/python3.10/dist-packages (from segmentation_models_pytorch) (2.5.1+cu121)\r\n",
      "Requirement already satisfied: torchvision>=0.9 in /usr/local/lib/python3.10/dist-packages (from segmentation_models_pytorch) (0.20.1+cu121)\r\n",
      "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.10/dist-packages (from segmentation_models_pytorch) (4.67.1)\r\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.24->segmentation_models_pytorch) (3.17.0)\r\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.24->segmentation_models_pytorch) (2024.12.0)\r\n",
      "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.24->segmentation_models_pytorch) (24.2)\r\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.24->segmentation_models_pytorch) (6.0.2)\r\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.24->segmentation_models_pytorch) (2.32.3)\r\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.24->segmentation_models_pytorch) (4.12.2)\r\n",
      "Requirement already satisfied: mkl_fft in /usr/local/lib/python3.10/dist-packages (from numpy>=1.19.3->segmentation_models_pytorch) (1.3.8)\r\n",
      "Requirement already satisfied: mkl_random in /usr/local/lib/python3.10/dist-packages (from numpy>=1.19.3->segmentation_models_pytorch) (1.2.4)\r\n",
      "Requirement already satisfied: mkl_umath in /usr/local/lib/python3.10/dist-packages (from numpy>=1.19.3->segmentation_models_pytorch) (0.1.1)\r\n",
      "Requirement already satisfied: mkl in /usr/local/lib/python3.10/dist-packages (from numpy>=1.19.3->segmentation_models_pytorch) (2025.0.1)\r\n",
      "Requirement already satisfied: tbb4py in /usr/local/lib/python3.10/dist-packages (from numpy>=1.19.3->segmentation_models_pytorch) (2022.0.0)\r\n",
      "Requirement already satisfied: mkl-service in /usr/local/lib/python3.10/dist-packages (from numpy>=1.19.3->segmentation_models_pytorch) (2.4.1)\r\n",
      "Collecting munch (from pretrainedmodels>=0.7.1->segmentation_models_pytorch)\r\n",
      "  Downloading munch-4.0.0-py2.py3-none-any.whl.metadata (5.9 kB)\r\n",
      "Requirement already satisfied: safetensors in /usr/local/lib/python3.10/dist-packages (from timm>=0.9->segmentation_models_pytorch) (0.4.5)\r\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.8->segmentation_models_pytorch) (3.4.2)\r\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.8->segmentation_models_pytorch) (3.1.4)\r\n",
      "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch>=1.8->segmentation_models_pytorch) (1.13.1)\r\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch>=1.8->segmentation_models_pytorch) (1.3.0)\r\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.8->segmentation_models_pytorch) (3.0.2)\r\n",
      "Requirement already satisfied: intel-openmp>=2024 in /usr/local/lib/python3.10/dist-packages (from mkl->numpy>=1.19.3->segmentation_models_pytorch) (2024.2.0)\r\n",
      "Requirement already satisfied: tbb==2022.* in /usr/local/lib/python3.10/dist-packages (from mkl->numpy>=1.19.3->segmentation_models_pytorch) (2022.0.0)\r\n",
      "Requirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.10/dist-packages (from tbb==2022.*->mkl->numpy>=1.19.3->segmentation_models_pytorch) (1.2.0)\r\n",
      "Requirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.10/dist-packages (from mkl_umath->numpy>=1.19.3->segmentation_models_pytorch) (2024.2.0)\r\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.24->segmentation_models_pytorch) (3.4.1)\r\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.24->segmentation_models_pytorch) (3.10)\r\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.24->segmentation_models_pytorch) (2.3.0)\r\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.24->segmentation_models_pytorch) (2025.1.31)\r\n",
      "Requirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.10/dist-packages (from intel-openmp>=2024->mkl->numpy>=1.19.3->segmentation_models_pytorch) (2024.2.0)\r\n",
      "Downloading segmentation_models_pytorch-0.4.0-py3-none-any.whl (121 kB)\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m121.3/121.3 kB\u001b[0m \u001b[31m5.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hDownloading munch-4.0.0-py2.py3-none-any.whl (9.9 kB)\r\n",
      "Building wheels for collected packages: efficientnet-pytorch, pretrainedmodels\r\n",
      "  Building wheel for efficientnet-pytorch (setup.py) ... \u001b[?25l\u001b[?25hdone\r\n",
      "  Created wheel for efficientnet-pytorch: filename=efficientnet_pytorch-0.7.1-py3-none-any.whl size=16424 sha256=8b508c15aeba4260e53703f096307dec9f304a26506b3a671f0708748f12e75c\r\n",
      "  Stored in directory: /root/.cache/pip/wheels/03/3f/e9/911b1bc46869644912bda90a56bcf7b960f20b5187feea3baf\r\n",
      "  Building wheel for pretrainedmodels (setup.py) ... \u001b[?25l\u001b[?25hdone\r\n",
      "  Created wheel for pretrainedmodels: filename=pretrainedmodels-0.7.4-py3-none-any.whl size=60944 sha256=a562ea62e508825f6e2d361c706300a8099376d141acc9effacf7efa6c13b334\r\n",
      "  Stored in directory: /root/.cache/pip/wheels/35/cb/a5/8f534c60142835bfc889f9a482e4a67e0b817032d9c6883b64\r\n",
      "Successfully built efficientnet-pytorch pretrainedmodels\r\n",
      "Installing collected packages: munch, efficientnet-pytorch, pretrainedmodels, segmentation_models_pytorch\r\n",
      "Successfully installed efficientnet-pytorch-0.7.1 munch-4.0.0 pretrainedmodels-0.7.4 segmentation_models_pytorch-0.4.0\r\n"
     ]
    }
   ],
   "source": [
    "!pip install segmentation_models_pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "058f4c0d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-13T12:23:58.032495Z",
     "iopub.status.busy": "2025-04-13T12:23:58.032262Z",
     "iopub.status.idle": "2025-04-13T12:24:17.658444Z",
     "shell.execute_reply": "2025-04-13T12:24:17.657530Z"
    },
    "papermill": {
     "duration": 19.632695,
     "end_time": "2025-04-13T12:24:17.659996",
     "exception": false,
     "start_time": "2025-04-13T12:23:58.027301",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os \n",
    "import cv2\n",
    "import numpy as np\n",
    "import torch\n",
    "import segmentation_models_pytorch as smp\n",
    "import matplotlib.pyplot as plt\n",
    "import torchvision.transforms as transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "54db4ec2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-13T12:24:17.669976Z",
     "iopub.status.busy": "2025-04-13T12:24:17.669663Z",
     "iopub.status.idle": "2025-04-13T12:24:17.749277Z",
     "shell.execute_reply": "2025-04-13T12:24:17.748516Z"
    },
    "papermill": {
     "duration": 0.085728,
     "end_time": "2025-04-13T12:24:17.750494",
     "exception": false,
     "start_time": "2025-04-13T12:24:17.664766",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3204a50d",
   "metadata": {
    "collapsed": true,
    "execution": {
     "iopub.execute_input": "2025-04-13T12:24:17.760715Z",
     "iopub.status.busy": "2025-04-13T12:24:17.760485Z",
     "iopub.status.idle": "2025-04-13T12:24:19.914596Z",
     "shell.execute_reply": "2025-04-13T12:24:19.913769Z"
    },
    "jupyter": {
     "outputs_hidden": true
    },
    "papermill": {
     "duration": 2.160652,
     "end_time": "2025-04-13T12:24:19.915990",
     "exception": false,
     "start_time": "2025-04-13T12:24:17.755338",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-4-0d348a2eee1c>:2: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(\"/kaggle/input/heart-segmwntation/pytorch/default/1/unet_heart_segmentation.pth\"))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Unet(\n",
       "  (encoder): ResNetEncoder(\n",
       "    (conv1): Conv2d(1, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
       "    (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (relu): ReLU(inplace=True)\n",
       "    (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
       "    (layer1): Sequential(\n",
       "      (0): BasicBlock(\n",
       "        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (1): BasicBlock(\n",
       "        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (2): BasicBlock(\n",
       "        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (layer2): Sequential(\n",
       "      (0): BasicBlock(\n",
       "        (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (downsample): Sequential(\n",
       "          (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "          (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (1): BasicBlock(\n",
       "        (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (2): BasicBlock(\n",
       "        (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (3): BasicBlock(\n",
       "        (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (layer3): Sequential(\n",
       "      (0): BasicBlock(\n",
       "        (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (downsample): Sequential(\n",
       "          (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "          (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (1): BasicBlock(\n",
       "        (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (2): BasicBlock(\n",
       "        (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (3): BasicBlock(\n",
       "        (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (4): BasicBlock(\n",
       "        (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (5): BasicBlock(\n",
       "        (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (layer4): Sequential(\n",
       "      (0): BasicBlock(\n",
       "        (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (downsample): Sequential(\n",
       "          (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "          (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (1): BasicBlock(\n",
       "        (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (2): BasicBlock(\n",
       "        (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (decoder): UnetDecoder(\n",
       "    (center): Identity()\n",
       "    (blocks): ModuleList(\n",
       "      (0): DecoderBlock(\n",
       "        (conv1): Conv2dReLU(\n",
       "          (0): Conv2d(768, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU(inplace=True)\n",
       "        )\n",
       "        (attention1): Attention(\n",
       "          (attention): Identity()\n",
       "        )\n",
       "        (conv2): Conv2dReLU(\n",
       "          (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU(inplace=True)\n",
       "        )\n",
       "        (attention2): Attention(\n",
       "          (attention): Identity()\n",
       "        )\n",
       "      )\n",
       "      (1): DecoderBlock(\n",
       "        (conv1): Conv2dReLU(\n",
       "          (0): Conv2d(384, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU(inplace=True)\n",
       "        )\n",
       "        (attention1): Attention(\n",
       "          (attention): Identity()\n",
       "        )\n",
       "        (conv2): Conv2dReLU(\n",
       "          (0): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU(inplace=True)\n",
       "        )\n",
       "        (attention2): Attention(\n",
       "          (attention): Identity()\n",
       "        )\n",
       "      )\n",
       "      (2): DecoderBlock(\n",
       "        (conv1): Conv2dReLU(\n",
       "          (0): Conv2d(192, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU(inplace=True)\n",
       "        )\n",
       "        (attention1): Attention(\n",
       "          (attention): Identity()\n",
       "        )\n",
       "        (conv2): Conv2dReLU(\n",
       "          (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU(inplace=True)\n",
       "        )\n",
       "        (attention2): Attention(\n",
       "          (attention): Identity()\n",
       "        )\n",
       "      )\n",
       "      (3): DecoderBlock(\n",
       "        (conv1): Conv2dReLU(\n",
       "          (0): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU(inplace=True)\n",
       "        )\n",
       "        (attention1): Attention(\n",
       "          (attention): Identity()\n",
       "        )\n",
       "        (conv2): Conv2dReLU(\n",
       "          (0): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU(inplace=True)\n",
       "        )\n",
       "        (attention2): Attention(\n",
       "          (attention): Identity()\n",
       "        )\n",
       "      )\n",
       "      (4): DecoderBlock(\n",
       "        (conv1): Conv2dReLU(\n",
       "          (0): Conv2d(32, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU(inplace=True)\n",
       "        )\n",
       "        (attention1): Attention(\n",
       "          (attention): Identity()\n",
       "        )\n",
       "        (conv2): Conv2dReLU(\n",
       "          (0): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU(inplace=True)\n",
       "        )\n",
       "        (attention2): Attention(\n",
       "          (attention): Identity()\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (segmentation_head): SegmentationHead(\n",
       "    (0): Conv2d(16, 5, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (1): Identity()\n",
       "    (2): Activation(\n",
       "      (activation): Identity()\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = smp.Unet(encoder_name=\"resnet34\", encoder_weights=None, in_channels=1, classes=5)\n",
    "model.load_state_dict(torch.load(\"/kaggle/input/heart-segmwntation/pytorch/default/1/unet_heart_segmentation.pth\"))\n",
    "model.to(device)\n",
    "model.eval() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3166aea4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-13T12:24:19.926794Z",
     "iopub.status.busy": "2025-04-13T12:24:19.926532Z",
     "iopub.status.idle": "2025-04-13T12:24:19.930349Z",
     "shell.execute_reply": "2025-04-13T12:24:19.929704Z"
    },
    "papermill": {
     "duration": 0.010485,
     "end_time": "2025-04-13T12:24:19.931564",
     "exception": false,
     "start_time": "2025-04-13T12:24:19.921079",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "transform = transforms.Compose([\n",
    "    transforms.ToPILImage(),\n",
    "    transforms.Grayscale(num_output_channels=1), \n",
    "    transforms.Resize((256, 256)), \n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.5], std=[0.5])  \n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3050aef3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-13T12:24:19.942114Z",
     "iopub.status.busy": "2025-04-13T12:24:19.941848Z",
     "iopub.status.idle": "2025-04-13T12:24:19.945313Z",
     "shell.execute_reply": "2025-04-13T12:24:19.944641Z"
    },
    "papermill": {
     "duration": 0.010033,
     "end_time": "2025-04-13T12:24:19.946483",
     "exception": false,
     "start_time": "2025-04-13T12:24:19.936450",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def preprocess_frame(frame):\n",
    "    gray_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "    input_tensor = transform(gray_frame).unsqueeze(0).to(device) \n",
    "    return input_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "12daa295",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-13T12:24:19.956967Z",
     "iopub.status.busy": "2025-04-13T12:24:19.956710Z",
     "iopub.status.idle": "2025-04-13T12:24:19.960735Z",
     "shell.execute_reply": "2025-04-13T12:24:19.960112Z"
    },
    "papermill": {
     "duration": 0.010492,
     "end_time": "2025-04-13T12:24:19.961865",
     "exception": false,
     "start_time": "2025-04-13T12:24:19.951373",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_segmentation_mask(frame):\n",
    "    input_tensor = preprocess_frame(frame)\n",
    "    with torch.no_grad():\n",
    "        output = model(input_tensor) \n",
    "        pred_mask = torch.argmax(output, dim=1).cpu().numpy()[0]  \n",
    "    h, w = frame.shape[:2]\n",
    "    pred_mask = cv2.resize(pred_mask.astype(np.uint8), (w, h), interpolation=cv2.INTER_NEAREST)\n",
    "    return pred_mask\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5619eef5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-13T12:24:19.972175Z",
     "iopub.status.busy": "2025-04-13T12:24:19.971947Z",
     "iopub.status.idle": "2025-04-13T12:24:19.975515Z",
     "shell.execute_reply": "2025-04-13T12:24:19.974930Z"
    },
    "papermill": {
     "duration": 0.009905,
     "end_time": "2025-04-13T12:24:19.976586",
     "exception": false,
     "start_time": "2025-04-13T12:24:19.966681",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_contours(mask, chamber_id):\n",
    "    chamber_mask = (mask == chamber_id).astype(np.uint8) * 255\n",
    "    contours, _ = cv2.findContours(chamber_mask, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "    \n",
    "    filtered_contours = [c for c in contours if cv2.contourArea(c) > 200]  \n",
    "    \n",
    "    return filtered_contours"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "36769efd",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-13T12:24:19.986781Z",
     "iopub.status.busy": "2025-04-13T12:24:19.986559Z",
     "iopub.status.idle": "2025-04-13T12:24:19.989745Z",
     "shell.execute_reply": "2025-04-13T12:24:19.989156Z"
    },
    "papermill": {
     "duration": 0.00945,
     "end_time": "2025-04-13T12:24:19.990855",
     "exception": false,
     "start_time": "2025-04-13T12:24:19.981405",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def calculate_polygon_area(contours):\n",
    "    total_area = 0\n",
    "    for contour in contours:\n",
    "        if len(contour) >= 3:\n",
    "            total_area += cv2.contourArea(contour)\n",
    "    return total_area"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "028dae42",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-13T12:24:20.001165Z",
     "iopub.status.busy": "2025-04-13T12:24:20.000940Z",
     "iopub.status.idle": "2025-04-13T12:24:20.004521Z",
     "shell.execute_reply": "2025-04-13T12:24:20.003926Z"
    },
    "papermill": {
     "duration": 0.009985,
     "end_time": "2025-04-13T12:24:20.005592",
     "exception": false,
     "start_time": "2025-04-13T12:24:19.995607",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def smooth_scale(value, ref):\n",
    "    if ref == 0:\n",
    "        return 0\n",
    "    scale_factor = value / ref\n",
    "\n",
    "    if scale_factor < 0.70:\n",
    "        scale_factor = 0.70 + (scale_factor - 0.70) * 0.001  \n",
    "    elif scale_factor > 1.30:\n",
    "        scale_factor = 1.30 - (scale_factor - 1.30) * 0.001\n",
    "\n",
    "    return round(scale_factor, 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7af8e216",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-13T12:24:20.016123Z",
     "iopub.status.busy": "2025-04-13T12:24:20.015853Z",
     "iopub.status.idle": "2025-04-13T12:24:20.023132Z",
     "shell.execute_reply": "2025-04-13T12:24:20.022393Z"
    },
    "papermill": {
     "duration": 0.014034,
     "end_time": "2025-04-13T12:24:20.024345",
     "exception": false,
     "start_time": "2025-04-13T12:24:20.010311",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def extract_smoothed_end_frames(video_path):\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    frame_idx = 0\n",
    "    raw_areas = []\n",
    "\n",
    "    while cap.isOpened():\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "        \n",
    "        mask = get_segmentation_mask(frame)\n",
    "        contours_dict = {ch: get_contours(mask, ch) for ch in range(1, 5)}\n",
    "        areas_for_frame = {ch: calculate_polygon_area(contours_dict[ch]) for ch in range(1, 5)}\n",
    "        total_area = sum(areas_for_frame.values())\n",
    "\n",
    "        raw_areas.append({\"Frame\": frame_idx, \"Total Area\": total_area, **areas_for_frame})\n",
    "        frame_idx += 1\n",
    "\n",
    "    cap.release()\n",
    "\n",
    "    if len(raw_areas) < 2:\n",
    "        raise ValueError(f\"Video {video_path} does not have enough frames to compute diastole and systole.\")\n",
    "\n",
    "    total_areas = [frame[\"Total Area\"] for frame in raw_areas]\n",
    "\n",
    "    end_diastole_idx = total_areas.index(max(total_areas))  \n",
    "    end_systole_idx = total_areas.index(min(total_areas[end_diastole_idx:]))\n",
    "\n",
    "    if end_systole_idx <= end_diastole_idx:\n",
    "        end_systole_idx = end_diastole_idx + 1\n",
    "        if end_systole_idx >= len(total_areas):\n",
    "            end_systole_idx = len(total_areas) - 1\n",
    "\n",
    "    stable_frames = raw_areas[end_diastole_idx:end_systole_idx]\n",
    "    if not stable_frames:\n",
    "        reference_frame = raw_areas[end_diastole_idx]\n",
    "    else:\n",
    "        reference_frame = min(stable_frames, key=lambda f: abs(f[\"Total Area\"] - np.mean(total_areas)))\n",
    "\n",
    "    scaled_dia = {}\n",
    "    scaled_sys = {}\n",
    "\n",
    "    for chamber_id in range(1, 5):\n",
    "        dia_value = raw_areas[end_diastole_idx][chamber_id]\n",
    "        sys_value = raw_areas[end_systole_idx][chamber_id]\n",
    "        ref_value = reference_frame[chamber_id] or 1 \n",
    "\n",
    "        scaled_dia[chamber_id] = smooth_scale(dia_value, ref_value)\n",
    "        scaled_sys[chamber_id] = smooth_scale(sys_value, ref_value)\n",
    "\n",
    "    return scaled_dia, scaled_sys\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0899fe57",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-13T12:24:20.034346Z",
     "iopub.status.busy": "2025-04-13T12:24:20.034125Z",
     "iopub.status.idle": "2025-04-13T12:24:20.039357Z",
     "shell.execute_reply": "2025-04-13T12:24:20.038598Z"
    },
    "papermill": {
     "duration": 0.011407,
     "end_time": "2025-04-13T12:24:20.040460",
     "exception": false,
     "start_time": "2025-04-13T12:24:20.029053",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def process_multiple_videos_smoothed(video_list):\n",
    "    all_dia = {1: [], 2: [], 3: [], 4: []}\n",
    "    all_sys = {1: [], 2: [], 3: [], 4: []}\n",
    "\n",
    "    for video_path in video_list:\n",
    "        scaled_dia, scaled_sys = extract_smoothed_end_frames(video_path)\n",
    "        for ch in range(1, 5):\n",
    "            all_dia[ch].append(scaled_dia[ch])\n",
    "            all_sys[ch].append(scaled_sys[ch])\n",
    "\n",
    "    avg_scaled_dia = {ch: round(np.mean(all_dia[ch]), 4) for ch in range(1, 5)}\n",
    "    avg_scaled_sys = {ch: round(np.mean(all_sys[ch]), 4) for ch in range(1, 5)}\n",
    "\n",
    "    chamber_names = {\n",
    "        1: \"Left Ventricle (LV)\",\n",
    "        2: \"Left Atrium (LA)\",\n",
    "        3: \"Right Ventricle (RV)\",\n",
    "        4: \"Right Atrium (RA)\"\n",
    "    }\n",
    "\n",
    "    print(\"\\n--- AVERAGE SMOOTHED SCALED END DIASTOLIC ---\")\n",
    "    for ch in range(1, 5):\n",
    "        print(f\"{chamber_names[ch]}: {avg_scaled_dia[ch]}\")\n",
    "\n",
    "    print(\"\\n--- AVERAGE SMOOTHED SCALED END SYSTOLIC ---\")\n",
    "    for ch in range(1, 5):\n",
    "        print(f\"{chamber_names[ch]}: {avg_scaled_sys[ch]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5b6696ce",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-13T12:24:20.050487Z",
     "iopub.status.busy": "2025-04-13T12:24:20.050257Z",
     "iopub.status.idle": "2025-04-13T12:24:20.084061Z",
     "shell.execute_reply": "2025-04-13T12:24:20.083461Z"
    },
    "papermill": {
     "duration": 0.040298,
     "end_time": "2025-04-13T12:24:20.085466",
     "exception": false,
     "start_time": "2025-04-13T12:24:20.045168",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "cluster_df = pd.read_csv(\"/kaggle/input/video-cluster/output_clusters.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ef34226c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-13T12:24:20.096011Z",
     "iopub.status.busy": "2025-04-13T12:24:20.095765Z",
     "iopub.status.idle": "2025-04-13T12:24:20.121479Z",
     "shell.execute_reply": "2025-04-13T12:24:20.120690Z"
    },
    "papermill": {
     "duration": 0.032182,
     "end_time": "2025-04-13T12:24:20.122838",
     "exception": false,
     "start_time": "2025-04-13T12:24:20.090656",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "cluster_2_videos = cluster_df[cluster_df['cluster'] == 2]['FileName'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9912928f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-13T12:24:20.133369Z",
     "iopub.status.busy": "2025-04-13T12:24:20.133139Z",
     "iopub.status.idle": "2025-04-13T12:24:20.136238Z",
     "shell.execute_reply": "2025-04-13T12:24:20.135460Z"
    },
    "papermill": {
     "duration": 0.009827,
     "end_time": "2025-04-13T12:24:20.137518",
     "exception": false,
     "start_time": "2025-04-13T12:24:20.127691",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "base_path = r\"/kaggle/input/echonet-dynamic/EchoNet-Dynamic/Videos\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ba49ed0b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-13T12:24:20.147597Z",
     "iopub.status.busy": "2025-04-13T12:24:20.147338Z",
     "iopub.status.idle": "2025-04-13T12:24:20.153543Z",
     "shell.execute_reply": "2025-04-13T12:24:20.152753Z"
    },
    "papermill": {
     "duration": 0.012593,
     "end_time": "2025-04-13T12:24:20.154782",
     "exception": false,
     "start_time": "2025-04-13T12:24:20.142189",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "video_list = [os.path.join(base_path, f\"{file}.avi\") for file in cluster_2_videos]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "99447a5c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-13T12:24:20.164907Z",
     "iopub.status.busy": "2025-04-13T12:24:20.164666Z",
     "iopub.status.idle": "2025-04-13T12:24:20.168858Z",
     "shell.execute_reply": "2025-04-13T12:24:20.168103Z"
    },
    "papermill": {
     "duration": 0.010694,
     "end_time": "2025-04-13T12:24:20.170089",
     "exception": false,
     "start_time": "2025-04-13T12:24:20.159395",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2648"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(video_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "dffbdae7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-13T12:24:20.180747Z",
     "iopub.status.busy": "2025-04-13T12:24:20.180542Z",
     "iopub.status.idle": "2025-04-13T13:29:12.754965Z",
     "shell.execute_reply": "2025-04-13T13:29:12.754031Z"
    },
    "papermill": {
     "duration": 3892.58733,
     "end_time": "2025-04-13T13:29:12.762336",
     "exception": false,
     "start_time": "2025-04-13T12:24:20.175006",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- AVERAGE SMOOTHED SCALED END DIASTOLIC ---\n",
      "Left Ventricle (LV): 1.1288\n",
      "Left Atrium (LA): 1.0405\n",
      "Right Ventricle (RV): 1.0669\n",
      "Right Atrium (RA): 0.989\n",
      "\n",
      "--- AVERAGE SMOOTHED SCALED END SYSTOLIC ---\n",
      "Left Ventricle (LV): 0.8694\n",
      "Left Atrium (LA): 0.8983\n",
      "Right Ventricle (RV): 0.8014\n",
      "Right Atrium (RA): 0.7966\n"
     ]
    }
   ],
   "source": [
    "process_multiple_videos_smoothed(video_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99b80d0a",
   "metadata": {
    "papermill": {
     "duration": 0.004579,
     "end_time": "2025-04-13T13:29:12.772241",
     "exception": false,
     "start_time": "2025-04-13T13:29:12.767662",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "datasetId": 4396335,
     "sourceId": 7548742,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 6951705,
     "sourceId": 11144031,
     "sourceType": "datasetVersion"
    },
    {
     "modelId": 276973,
     "modelInstanceId": 255637,
     "sourceId": 299051,
     "sourceType": "modelInstanceVersion"
    }
   ],
   "dockerImageVersionId": 30919,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 3933.239759,
   "end_time": "2025-04-13T13:29:16.120560",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2025-04-13T12:23:42.880801",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
